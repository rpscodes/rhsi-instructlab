= Solution Pattern: Name Template
:sectnums:
:sectlinks:
:doctype: book

= See the Solution in Action

== Demonstration

References:

* https://github.com/instructlab[Commands extract from the InstructLab project]
* https://developers.redhat.com/blog/2024/06/12/getting-started-instructlab-generative-ai-model-tuning#model_alignment_and_training_with_instructlab[Getting started with InstructLab: Generative AI model tuning]
* https://github.com/instructlab/instructlab/blob/main/README.md#-installing-ilab[InstructLab installation guide]

== Concepts and Commands Used in the Demonstration

[NOTE]
====
Please make sure to explain what each Skupper command does the first time you use it, especially for people unfamiliar with Skupper. The following commands should be explained:

- **`skupper init`**: Initializes the Skupper network, setting up the necessary components to enable secure communication between services.
- **`skupper expose`**: Exposes a local service through the Skupper network, allowing it to be accessed from other Skupper-connected sites.
- **`skupper token`**: Generates a connection token that can be used by other sites to link to the Skupper network, ensuring secure communication.
- **`skupper link`**: Establishes a secure link between two Skupper sites using the token created by `skupper token`.
- **`skupper service status`**: Displays the status of services exposed through Skupper, showing what is accessible and how it’s connected within the network.
- **`ilab download`**: Downloads the model to be used by the chatbot.
- **`ilab model serve`**: Starts the server that will be responsible for receiving the user input, sending it to the LLaMA3 model, and sending the response back to the user.
- **`ilab model chat`**: Starts the chatbot, allowing the user to interact with it.

====


== Run the demonstration

=== AI Model Deployment with InstructLab

The first step is to deploy the InstructLab chat model in the InstructLab site. The InstructLab chat model will be responsible for receiving the user input and sending it to the LLaMA3 model. The response from the LLaMA3 model will be sent back to the user. This is based on the article: https://developers.redhat.com/blog/2024/06/12/getting-started-instructlab-generative-ai-model-tuning#model_alignment_and_training_with_instructlab[Getting Started with InstructLab for Generative AI Model Tuning].

[.console-input]
[source,shell script]
----
mkdir instructlab && cd instructlab
python3.11 -m venv venv
source venv/bin/activate
pip install 'instructlab[cuda]' -C cmake.args="-DLLAMA_CUDA=on" -C cmake.args="-DLLAMA_NATIVE=off"
----

[IMPORTANT]
====
This installation method will enable your Nvidia GPU to be used by InstructLab. If you don't have an Nvidia GPU, please check other options in: https://github.com/instructlab/instructlab/blob/main/README.md#-installing-ilab[InstructLab Installation Guide].
====


=== Initialize the InstructLab Configuration

After installing InstructLab, you need to initialize the configuration. This is the step where you create the `config.yaml` file, at `~/instructlab/config.yaml`, with the default configuration. To initialize the configuration, run the following command:

[.console-input]
[source,shell script]
----
ilab config init
----

To enable external access to your model, please modify the `config.yaml` file, located into your `instructlab` directory ie. `~/instructlab/config.yaml`, with the following content:

[source,yaml]
----
chat:
  context: default
  greedy_mode: false
  logs_dir: data/chatlogs
  max_tokens: null
  model: models/merlinite-7b-lab-Q4_K_M.gguf
  session: null
  vi_mode: false
  visible_overflow: true
general:
  log_level: INFO
generate:
  chunk_word_count: 1000
  model: models/merlinite-7b-lab-Q4_K_M.gguf
  num_cpus: 10
  num_instructions: 100
  output_dir: generated
  prompt_file: prompt.txt
  seed_file: seed_tasks.json
  taxonomy_base: origin/main
  taxonomy_path: taxonomy
serve:
  gpu_layers: -1
  host_port: 0.0.0.0:8000
  max_ctx_size: 4096
  model_path: models/merlinite-7b-lab-Q4_K_M.gguf
----

=== Download the model

Now you need to download the model, this is the step where you download the model to be used by the chatbot. The model is the LLaMA3 model, which is a large language model trained on the Merlinite dataset. The model is responsible for generating the responses to the user input. To download the model, run the following command:

[.console-input]
[source,shell script]
----
ilab model download
----

=== Start the server

The last step is to start the server. The server will be responsible for receiving the user input, sending it to the LLaMA3 model, and sending the response back to the user.

[.console-input]
[source,shell script]
----
ilab model serve

# The output should be similar to:
INFO 2024-07-30 18:59:01,199 serve.py:51: serve Using model 'models/merlinite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-07-30 18:59:01,611 server.py:218: server Starting server process, press CTRL+C to shutdown server...
INFO 2024-07-30 18:59:01,612 server.py:219: server After application startup complete see http://0.0.0.0:8000/docs for API.
----

== Private Skupper Deployment

The second step is to deploy the private Skupper in Site A. The private Skupper will be responsible for creating a secure connection between the two sites, allowing the Ollama Pilot application to send requests to the LLaMA3 model and receive the response from the merlinite model. Open a new terminal and run the following commands:

=== Install Skupper

To install skupper on site A, with podman as the platform:

[.console-input]
[source,shell script]
----
export SKUPPER_PLATFORM=podman
skupper init --ingress none
----


[NOTE]
====
* `SKUPPER_PLATFORM=podman` is used to set the platform to podman. This is necessary because the private Skupper will be running on a podman container.
* `skupper init` is used to initialize the Skupper network, setting up the necessary components to enable secure communication between services.
* The `--ingress none` flag is used to disable the automatic creation of an ingress controller. This is necessary because the public Skupper will be responsible for exposing the service to the internet.
====

=== Exposing the InstructLab Chat Model

To bind the local service running the InstructLab chat model to the Skupper service:

[.console-input]
[source,shell script]
----
skupper expose host host.containers.internal --address instructlab --port 8000
----

[NOTE]
====
* `skupper expose` is used to expose a local service through the Skupper network, allowing it to be accessed from other Skupper-connected sites.
* `host.containers.internal` is used to bind the local service to the Skupper service.
* `--address instructlab` is used to specify the address of the service.
* `--port 8000` is used to specify the port of the service.
====

Check the status of the Skupper service:

[.console-input]
[source,shell script]
----
skupper service status

Services exposed through Skupper:
╰─ instructlab:8000 (tcp)
----

[NOTE]
====
* `skupper service status` is used to display the status of services exposed through Skupper, showing what is accessible and how it’s connected within the network.
====

== Public Skupper Deployment

Deploy the public Skupper in Site B. The public Skupper will receive the connection from the private Skupper and create a secure connection between the two sites.

=== Creating the project and deploying the public Skupper:

This is the step where you create the project and deploy the public Skupper. The public Skupper will be responsible for receiving the connection from the private Skupper and creating a secure connection between the two sites. Open a new terminal and run the following commands:


[.console-input]
[source,shell script]
----
export SKUPPER_PLATFORM=kubernetes
oc new-project ollama-pilot
skupper init --enable-console --enable-flow-collector --console-user admin --console-password admin
----

[IMPORTANT]
====
* Run this command in a new terminal and keep it open, because the default platform is `kubernetes` and the private terminal is using `podman`.
====

[NOTE]
====
* `SKUPPER_PLATFORM=kubernetes` is used to set the platform to Kubernetes. This is necessary because the public Skupper will be running on a Kubernetes cluster.
* `oc new-project ollama-pilot` is used to create a new project called `ollama-pilot`.
* `skupper init` is used to initialize the Skupper network, setting up the necessary components to enable secure communication between services.
* The `--enable-console` flag is used to enable the Skupper console, which provides a web interface for managing the Skupper network.
* The `--enable-flow-collector` flag is used to enable the flow collector, which collects and displays information about the traffic flowing through the Skupper network.
* The `--console-user admin` flag is used to set the username for the Skupper console to `admin`.
* The `--console-password admin` flag is used to set the password for the Skupper console to `admin`.
====

=== Creating the token to allow the private Skupper to connect to the public Skupper:

This is the step where you create the token to allow the private Skupper to connect to the public Skupper. Open a new terminal and run the following command:

[.console-input]
[source,shell script]
----
skupper token create token.yaml
----

[NOTE]
====
* `skupper token create token.yaml` is used to generate a connection token that can be used by other sites to link to the Skupper network, ensuring secure communication.
* The `token.yaml` file will contain the token to connect the two sites.
====

Now, you'll have a `token.yaml` file with the token to connect the two sites.

=== Secure Communication Between the Two Sites with Skupper

Now it's time to establish a secure connection between the two sites using the token created by the public Skupper. At the terminal where the private Skupper is running, run the following command to link the two sites:

[.console-input]
[source,shell script]
----
skupper link create token.yaml --name instructlab
----

[NOTE]
====
* `skupper link create token.yaml --name instructlab` is used to establish a secure link between two Skupper sites using the token created by `skupper token`.
====

Check the status of the Skupper link:

[.console-input]
[source,shell script]
----
skupper link status

Links created from this site:

        Link instructlab is connected

Current links from other sites that are connected:

        There are no connected links
----

[NOTE]
====
* `skupper link status` is used to display the status of the links created by the Skupper network, showing which sites are connected and how they are connected.
====

Check the status on the public Skupper terminal:

[.console-input]
[source,shell script]
----
skupper link status

Links created from this site:

       There are no links configured or connected

Current links from other sites that are connected:

       Incoming link from site b8ad86d5-9680-4fea-9c07-ea7ee394e0bd
----

[NOTE]
====
* `skupper link status` is used to display the status of the links created by the Skupper network, showing which sites are connected and how they are connected.
====

=== Chatbot with Protected Data

The last step is to expose the service in the public Skupper and create the Ollama Pilot application.

* Still on the terminal where the public Skupper is running, run the following command to expose the service:

[.console-input]
[source,shell script]
----
skupper service create instructlab 8000
----
* Exposing the service to the internet:

[.console-input]
[source,shell script]
----
oc expose service instructlab
----

[NOTE]
====
* `skupper service create instructlab 8000` is used to create a service in the public Skupper, allowing it to be accessed from the private Skupper.
* `oc expose service instructlab` is used to expose the service to the internet, allowing it to be accessed by the Ollama Pilot application.
====

* Getting the public URL:

This URL will be used to access the chatbot from the Ollama Pilot application.

[.console-input]
[source,shell script]
----
oc get route instructlab
NAME          HOST/PORT                                      PATH          SERVICES                PORT       TERMINATION   WILDCARD
instructlab   instructlab-ollama-pilot.apps.your-cluster-url instructlab                           port8000   None
----

[NOTE]
====
* `oc get route instructlab` is used to get the public URL of the service, which will be used to access the chatbot from the Ollama Pilot application.
====

== Finally, to interact with the chatbot

Let's interact with the chatbot. Run the following command to start the chatbot:

[.console-input]
[source,shell script]
----
ilab model chat --endpoint-url http://instructlab-ollama-pilot.apps.your-cluster-url/v1/

╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ MODELS/MERLINITE-7B-LAB-Q4_K_M.GGUF (type /h for help)                                                                                                                                                                                                                                      │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
>>> Hello, who are you?                                                                                                                                                                                                                                                                                           [S][default]
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── models/merlinite-7b-lab-Q4_K_M.gguf ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ I am an AI Language Model from IBM Research, trained to assist with various tasks such as answering questions, translating text, summarizing long documents, and even explaining complex concepts. I am here to help make your life easier!                                                                                │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── elapsed 14.291 seconds ─╯
>>> What is Red Hat Service Interconnect                                                                                                                                                                                                                                                                          [S][default]
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── models/merlinite-7b-lab-Q4_K_M.gguf ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Red Hat Service Interconnect (RHSI) is a software-defined networking solution that allows organizations to create secure, high-performance networks between their on-premises environments and various cloud resources. It enables seamless communication between different data centers, public clouds (such as Amazon    │
│ Web Services or Microsoft Azure), and even edge devices (such as IoT gateways or mobile applications). RHSI offers the following key features:                                                                                                                                                                             │
│                                                                                                                                                                                                                                                                                                                            │
│ 1. **Security**: RHSI ensures secure communication between different environments by employing advanced encryption algorithms like IPSEC and TLS. It also supports virtual private cloud (VPC) concepts for creating isolated networking environments within public clouds.                                                │
│ 2. **Performance**: RHSI provides high-bandwidth, low-latency network connections. This makes it suitable for handling large data transfers or real-time applications like video streaming and gaming.                                                                                                                     │
│ 3. **Flexibility**: RHSI supports a wide range of networking protocols such as IPv4, IPv6, and even overlay networks (such as OpenFlow). This allows organizations to build complex networking topologies that cater to their specific needs.                                                                              │
│ 4. **Automation**: RHSI offers programmable network interfaces, enabling automated provisioning and management of networking resources. This reduces the administrative overhead and improves operational efficiency.                                                                                                      │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── elapsed 94.969 seconds ─╯
>>>              
----

[NOTE]
====
* `ilab model chat --endpoint-url http://instructlab-ollama-pilot.apps.your-cluster-url/v1/` is used to start the chatbot, allowing the user to interact with it.
* The chatbot will respond to the user input by sending it to the LLaMA3 model and returning the response back to the user.
====

